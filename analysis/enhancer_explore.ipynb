{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Enhancers!\n",
    "\n",
    "This notebook performs downstream analysis using the results from the DNABERT-Enhancer model. In particular, the noteook uses the provided FPR thresholds and classification results to cluster the enhancers into three categories: active, poised enhancers active dissimilar (PEADS), and poised enhancers active similar (peas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pybedtools import BedTool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the necessary numpy arrays from the evaluation function in `model_src`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26157 positive samples being tested.\n",
      "There are 15722 poised enhancers\n",
      "There are 10435 active enhancers\n"
     ]
    }
   ],
   "source": [
    "# Load in the Dataset\n",
    "predictions_dir = \"../output/positive_evaulation/\"\n",
    "yground = np.load(predictions_dir + \"labels.npy\")\n",
    "ylogits = np.load(predictions_dir + \"pred_results.npy\")\n",
    "print(f'There are {yground.shape[0]} positive samples being tested.')\n",
    "num_samples = yground.shape[0]\n",
    "\n",
    "print('There are {} poised enhancers'.format(np.count_nonzero(yground == 0))) # num poised\n",
    "print('There are {} active enhancers'.format(np.count_nonzero(yground == 2))) # num active "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Model Results Using FPR Thresholds\n",
    "The accuracy we received is in line with our expectations considering our large sample of negative samples, and the hypothesized similarity between some poised and active enhancers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary Store for FPR Thresholds \n",
    "## Load this from Eval Results JSON file. \n",
    "POISED_THRESHOLDS = {\n",
    "    0.10 : -0.39354804158210754,\n",
    "    0.05 : 0.0886303037405014,\n",
    "    0.03 : 0.34452739357948303,\n",
    "    0.01 : 0.8528411388397217,\n",
    "}\n",
    "\n",
    "ACTIVE_THRESHOLDS = {\n",
    "    0.10 : -0.5178720355033875,\n",
    "    0.05 : -0.11259077489376068,\n",
    "    0.03 : 0.170790046453475952,\n",
    "    0.01 : 0.7585378885269165,\n",
    "}\n",
    "\n",
    "def predict(logits, fpr):\n",
    "    assert fpr in [0.10, 0.05, 0.03, 0.01]\n",
    "\n",
    "    pe_thresh = POISED_THRESHOLDS[fpr]\n",
    "    ae_thresh = ACTIVE_THRESHOLDS[fpr]\n",
    "    if logits[0] > pe_thresh and logits[2] > ae_thresh:\n",
    "        return 0 if logits[0] > logits[2] else 2\n",
    "    elif logits[0] > pe_thresh:\n",
    "        return 0\n",
    "    elif logits[2] > ae_thresh:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Print Out Overall Accuracy\n",
    "fpr = 0.05\n",
    "ypreds = np.apply_along_axis(predict, 1, ylogits, fpr)\n",
    "np.save(os.path.join(\"../output/positive_evaulation/\", \"ypreds_fpr05.npy\"), ypreds)\n",
    "usable_ypreds = np.where(ypreds != 1)[0]\n",
    "\n",
    "fpr_thresh_acc = np.count_nonzero(ypreds == yground) / len(yground)\n",
    "print(f'Accuracy accounting for FPR: {fpr_thresh_acc}')\n",
    "\n",
    "valid_pred = ypreds != 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting AE Scores Across Poised and Active Enhancers\n",
    "\n",
    "Plotting the AE scores for AEs and PEs that were not clasified as noise. We keep misclasified (between PEs and AEs) due to the high AUC scores and the high similarity between AEs and PEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Function Based on Logits - should see clustering. \n",
    "def scale(logits):\n",
    "    norm = np.linalg.norm(logits, 1)\n",
    "    return logits / norm\n",
    "\n",
    "def plot_enhancer(logits, bin_width, enhancer_type, normalized):\n",
    "    assert enhancer_type in [\"Active\", \"Poised\"]\n",
    "\n",
    "    # Define sampls to bin and count\n",
    "    bins = np.arange(np.min(logits), np.max(logits), bin_width)\n",
    "    bin_counts, _ = np.histogram(logits, bins=bins)\n",
    "\n",
    "    plt.bar(bins[:-1], bin_counts, width=bin_width, align='edge', alpha=0.4, color='blue')\n",
    "    plt.plot(bins[:-1] + bin_width/2, bin_counts, color='blue')\n",
    "\n",
    "    plt.xlabel('Acitve Enhancer Score', fontsize=10)\n",
    "    plt.ylabel(f'Number of {enhancer_type} Enhancers', fontsize=10)\n",
    "    plt.xticks(fontsize=9.2)\n",
    "    plt.yticks(fontsize=9.2)\n",
    "    if normalized:\n",
    "        plt.title(f'Normalized Active Enhancer Score versus Number of {enhancer_type} Enhancers', fontsize=12)\n",
    "    else:\n",
    "        plt.title(f'Unnormalized Active Enhancer Score versus Number of {enhancer_type} Enhancers', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_enhancer_special(logits, bin_width, enhancer_type, normalized):\n",
    "    assert enhancer_type in [\"Active\", \"Poised\"]\n",
    "\n",
    "    # Define samples to bin and count\n",
    "    bins = np.arange(np.min(logits), np.max(logits), bin_width)\n",
    "    bin_counts, _ = np.histogram(logits, bins=bins)\n",
    "\n",
    "    # Split the bins and bin_counts into two groups: values under 0 and values greater than or equal to 0\n",
    "    negative_mask = bins[:-1] < 0\n",
    "    positive_mask = bins[:-1] >= 0\n",
    "\n",
    "    # Plot the bars for negative values (color: orange)\n",
    "    plt.bar(bins[:-1][negative_mask], bin_counts[negative_mask], width=bin_width, align='edge', alpha=0.4, color='orange')\n",
    "\n",
    "    # Plot the bars for non-negative values (color: green)\n",
    "    plt.bar(bins[:-1][positive_mask], bin_counts[positive_mask], width=bin_width, align='edge', alpha=0.4, color='green')\n",
    "\n",
    "    # Plot the line plot for negative values (color: orange)\n",
    "    plt.plot(bins[:-1][negative_mask] + bin_width/2, bin_counts[negative_mask], color='orange')\n",
    "\n",
    "    # Plot the line plot for non-negative values (color: green)\n",
    "    plt.plot(bins[:-1][positive_mask] + bin_width/2, bin_counts[positive_mask], color='green')\n",
    "\n",
    "    plt.xlabel('Active Enhancer Score', fontsize=10)\n",
    "    plt.ylabel(f'Number of {enhancer_type} Enhancers', fontsize=10)\n",
    "    plt.xticks(fontsize=9.2)\n",
    "    plt.yticks(fontsize=9.2)\n",
    "    if normalized:\n",
    "        plt.title(f'Normalized Active Enhancer Score versus Number of {enhancer_type} Enhancers', fontsize=12)\n",
    "    else:\n",
    "        plt.title(f'Unnormalized Active Enhancer Score versus Number of {enhancer_type} Enhancers', fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_condition = np.logical_and(yground == 2, valid_pred)\n",
    "active_indices = np.where(active_condition)\n",
    "active_logits = ylogits[active_indices]\n",
    "plot_enhancer(active_logits[:,2], 0.025, \"Active\", False)\n",
    "\n",
    "active_normed = np.apply_along_axis(scale, 1, np.delete(active_logits, 1, axis=1))\n",
    "plot_enhancer(active_normed[:,1], 0.025, \"Active\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poised_condition = np.logical_and(yground == 0, valid_pred)\n",
    "poised_indices = np.where(poised_condition)\n",
    "\n",
    "poised_logits = ylogits[poised_indices]\n",
    "plot_enhancer(poised_logits[:,2], 0.025, \"Poised\", False)\n",
    "\n",
    "poised_normed = np.apply_along_axis(scale, 1, np.delete(poised_logits, 1, axis=1))\n",
    "plot_enhancer_special(poised_normed[:,1], 0.025, \"Poised\", True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in all of the Transcription Factor Binding Sites that We Have in a BED File in addition to our positive BED dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_file = \"../analysis/results/comparison_files/EFO-0003042.TFbinding.bed\"\n",
    "tf_sites = list(BedTool(bed_file))\n",
    "\n",
    "tfsites = []\n",
    "with open(bed_file, 'r') as file:\n",
    "    for line in file:\n",
    "        info = line.split()\n",
    "        tfsites.append(info)\n",
    "\n",
    "df_tfsites = pd.DataFrame(tfsites, columns=['Chrom', 'Start', 'End', 'TF'])\n",
    "\n",
    "from pybedtools import BedTool\n",
    "\n",
    "positive_bed = \"../data/positive.bed\"\n",
    "positive_list = list(BedTool(positive_bed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_logits = np.apply_along_axis(scale, 1, np.delete(ylogits, 1, axis=1))\n",
    "active_i = active_indices[0]\n",
    "peads_i = np.where(np.logical_and(poised_condition, normalized_logits[:,1] < 0.0))[0]\n",
    "peas_i = np.where(np.logical_and(poised_condition, normalized_logits[:,1] >= 0.0))[0]\n",
    "\n",
    "assert(len(active_i) + len(peas_i) + len(peads_i) == np.count_nonzero(ypreds != 1))\n",
    "\n",
    "active_b = [positive_list[i] for i in active_i]\n",
    "peads_b = [positive_list[i] for i in peas_i]\n",
    "peas_b = [positive_list[i] for i in peads_i]\n",
    "\n",
    "active_bed = BedTool(active_b)\n",
    "peads_bed = BedTool(peas_b)\n",
    "peas_bed = BedTool(peads_b)\n",
    "\n",
    "active_bed.saveas('../data/active.bed')\n",
    "peads_bed.saveas('../data/peads.bed')\n",
    "peas_bed.saveas('../data/peas.bed')\n",
    "\n",
    "num_active = len(active_i)\n",
    "num_peads = len(peads_i)\n",
    "num_peas = len(peas_i)\n",
    "\n",
    "sizes = {\"Active\" : num_active, \"PEADS\" : num_peads, \"PEAS\" : num_peas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Positive Index to Each of the BED files. \n",
    "\n",
    "def add_index_to_bed(input_file, indices=None, custom_indices=False):\n",
    "    with open(input_file, \"r\") as f:\n",
    "        bed_lines = f.readlines()\n",
    "    if custom_indices:\n",
    "        bed_lines_with_index = [\n",
    "            f\"{line[:-1]}\\t{i}\\n\" for i, line in zip(indices, bed_lines)\n",
    "        ]\n",
    "    else:\n",
    "        bed_lines_with_index = [\n",
    "            f\"{line[:-1]}\\t{i}\\n\" for i, line in enumerate(bed_lines)\n",
    "        ]\n",
    "\n",
    "    with open(f\"{input_file}.indexed\", \"w\") as f:\n",
    "        f.writelines(bed_lines_with_index)\n",
    "\n",
    "add_index_to_bed(\"../data/active.bed\", indices=active_i, custom_indices=True)\n",
    "add_index_to_bed(\"../data/peads.bed\", indices=peads_i, custom_indices=True)\n",
    "add_index_to_bed(\"../data/peas.bed\", indices=peas_i, custom_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data for the three groups\n",
    "group_names = ['Active', 'PEADS', 'PEAS']\n",
    "group_counts = [num_active, num_peads, num_peas]\n",
    "\n",
    "# Use Seaborn's \"pastel\" palette\n",
    "sns.set_palette('pastel')\n",
    "\n",
    "# Create the pie chart\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def autopct_format(values):\n",
    "    def my_format(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct * total / 100.0))\n",
    "        return '{v:d}'.format(v=val)\n",
    "    return my_format\n",
    "\n",
    "plt.pie(group_counts, labels=group_names, autopct=autopct_format(group_counts), textprops={'fontsize': 10}, startangle=90)\n",
    "\n",
    "# Add a title\n",
    "ax.set_title('Counts of Each Enhancer Type from Positive Dataset', fontsize=12)\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle\n",
    "ax.axis('equal')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Enhancer Categories' Enrichment Across Different Transcription Factors\n",
    "Data was acquired from around 80 transcription factors from the ENCODE database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in BED Intersect Files\n",
    "def read_bed(file):\n",
    "    sites = []\n",
    "    with open(file, 'r') as file:\n",
    "        for line in file:\n",
    "            info = line.split()\n",
    "            sites.append(info)\n",
    "    return sites\n",
    "\n",
    "tfactive = read_bed(\"../analysis/results/bed_files/tfbind_active.bed\")\n",
    "tfpeads = read_bed(\"../analysis/results/bed_files/tfbind_peads.bed\")\n",
    "tfpeas = read_bed(\"../analysis/results/bed_files/tfbind_peas.bed\")\n",
    "\n",
    "print(len(tfactive), len(tfpeads), len(tfpeas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explore_utils import process_intersection, determine_fold_change\n",
    "from explore_utils import plot_histogram\n",
    "\n",
    "## Process BEDS\n",
    "map_active, _ = process_intersection(tfactive)\n",
    "map_poised, _ = process_intersection(tfpeads)\n",
    "map_p_to_a, _ = process_intersection(tfpeas)\n",
    "\n",
    "## Set the Exclusion Bound (i.e. how many standard deviations)\n",
    "STDEV_EX = 2.5\n",
    "\n",
    "## Active vs Poised\n",
    "avsp_map = determine_fold_change(map_active, map_poised, \"Active\", \"PEADS\", sizes)\n",
    "# del avsp_map['CBX8']\n",
    "del avsp_map['POLR2AphosphoS5']\n",
    "plot_histogram(avsp_map, \"Active\", \"PEADS\", STDEV_EX)\n",
    "\n",
    "## Active vs PoisedtoActive\n",
    "avspa_map = determine_fold_change(map_active, map_p_to_a, \"Active\", \"PEAS\", sizes)\n",
    "del avspa_map['CBX8']\n",
    "del avspa_map['POLR2AphosphoS5']\n",
    "plot_histogram(avspa_map, \"Active\", \"PEAS\", STDEV_EX)\n",
    "\n",
    "## Poised vs Poised_to_Active\n",
    "pvspa_map = determine_fold_change(map_poised, map_p_to_a, \"PEADS\", \"PEAS\", sizes)\n",
    "del pvspa_map['POLR2AphosphoS5']\n",
    "# plot_histogram(pvspa_map, \"PEADS\", \"PEAS\", STDEV_EX)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlating Enhancer Categories with Specific Cell Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../analysis/results/bed_files/active_cellintersect.bed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Import Intersect Files generated using Bedtools Intersect\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ae_cellx \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39m../analysis/results/bed_files/active_cellintersect.bed\u001b[39;49m\u001b[39m\"\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m ae_cellx\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mChrom\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mStart\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEnd\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCellID\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m peads_cellx \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m../analysis/results/bed_files/peads_cellintersect.bed\u001b[39m\u001b[39m\"\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/users/pranavmahableshwarkar/miniconda3_x86_64/envs/biolearning/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/users/pranavmahableshwarkar/miniconda3_x86_64/envs/biolearning/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/users/pranavmahableshwarkar/miniconda3_x86_64/envs/biolearning/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/users/pranavmahableshwarkar/miniconda3_x86_64/envs/biolearning/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/users/pranavmahableshwarkar/miniconda3_x86_64/envs/biolearning/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../analysis/results/bed_files/active_cellintersect.bed'"
     ]
    }
   ],
   "source": [
    "# Import Intersect Files generated using Bedtools Intersect\n",
    "ae_cellx = pd.read_csv(\"../analysis/results/bed_files/active_cellintersect.bed\", sep=\"\\t\")\n",
    "ae_cellx.columns = ['Chrom', 'Start', 'End', 'CellID']\n",
    "\n",
    "peads_cellx = pd.read_csv(\"../analysis/results/bed_files/peads_cellintersect.bed\", sep=\"\\t\")\n",
    "peads_cellx.columns = ['Chrom', 'Start', 'End', 'CellID']\n",
    "\n",
    "peas_cellx = pd.read_csv(\"../analysis/results/bed_files/peas_cellintersect.bed\", sep=\"\\t\")\n",
    "peas_cellx.columns = ['Chrom', 'Start', 'End', 'CellID']\n",
    "\n",
    "# Load in Annotation File on 107 Different Cell Types\n",
    "cellid_annot = pd.read_csv(\"../analysis/results/comparison_files/all27ac.cell.anno\", sep='\\t', index_col=0, header=None)\n",
    "cellid_annot.columns=['CellID', 'CellName', 'CellCategory', 'StemProg']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Distribution of PEADS and PEAS Across Cell Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_counts = ae_cellx['CellID'].value_counts() / len(ae_cellx.index)\n",
    "cellid_annot['AEFreq'] = ae_counts\n",
    "\n",
    "peads_counts = peads_cellx['CellID'].value_counts() / len(peads_cellx.index)\n",
    "cellid_annot['PEADSFreq'] = peads_counts\n",
    "\n",
    "peas_counts = peas_cellx['CellID'].value_counts() / len(peas_cellx.index)\n",
    "cellid_annot['PEASFreq'] = peas_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "sns.set(font_scale=0.6)\n",
    "\n",
    "\n",
    "unique_cells = cellid_annot['CellName']\n",
    "x = list(range(len(unique_cells)))\n",
    "bar_width = 0.4\n",
    "\n",
    "plt.bar(x, cellid_annot['PEADSFreq'], width=bar_width, color='blue', label='PEADS Frequency')\n",
    "# Plot the bars for DataFrame 2 (orange bars)\n",
    "plt.bar([i + bar_width for i in x], cellid_annot['PEASFreq'], width=bar_width, color='orange', label='PEAS Frequency')\n",
    "\n",
    "plt.xticks([i + bar_width / 2 for i in x], unique_cells, rotation=60, ha='right') \n",
    "plt.xlabel('Cell Type')\n",
    "plt.ylabel('Category Frequency')\n",
    "plt.title('Distribution of CAEs for PEADs and PEASs')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAttr(cellID, attr):\n",
    "    value = cellid_annot.loc[cellID]\n",
    "    return value[attr]\n",
    "\n",
    "ae_cellx['StemProg'] = ae_cellx['CellID'].map(lambda x: findAttr(x, 'StemProg'), na_action=None)\n",
    "ae_stemprog_counts = ae_cellx['StemProg'].value_counts() / len(ae_cellx.index)\n",
    "\n",
    "peads_cellx['StemProg'] = peads_cellx['CellID'].map(lambda x: findAttr(x, 'StemProg'), na_action=None)\n",
    "peads_stemprog_counts = peads_cellx['StemProg'].value_counts() / len(peads_cellx.index)\n",
    "\n",
    "peas_cellx['StemProg'] = peas_cellx['CellID'].map(lambda x: findAttr(x, 'StemProg'), na_action=None)\n",
    "peas_stemprog_counts = peas_cellx['StemProg'].value_counts() / len(peas_cellx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [ae_stemprog_counts, peads_stemprog_counts, peas_stemprog_counts]\n",
    "values_normal = [count['normal'] for count in counts]\n",
    "values_cancer = [count['cancer'] for count in counts]\n",
    "values_stem = [count['stem'] for count in counts]\n",
    "values_progenitor = [count['progenitor'] for count in counts]\n",
    "\n",
    "# Get the unique categories for the x-axis labels\n",
    "categories = ['AE', 'PEADS', 'PEAS']\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.2\n",
    "\n",
    "# Set the positions for the bars on the x-axis\n",
    "x_normal = range(len(categories))\n",
    "x_cancer = [i + bar_width for i in x_normal]\n",
    "x_stem = [i + bar_width*2 for i in x_normal]\n",
    "x_progenitor = [i + bar_width*3 for i in x_normal]\n",
    "\n",
    "# Create the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_normal, values_normal, width=bar_width, color='blue', label='Normal')\n",
    "plt.bar(x_cancer, values_cancer, width=bar_width, color='orange', label='Cancer')\n",
    "plt.bar(x_stem, values_stem, width=bar_width, color='green', label='Stem')\n",
    "plt.bar(x_progenitor, values_progenitor, width=bar_width, color='red', label='Progenitor')\n",
    "\n",
    "# Set x-axis labels and ticks\n",
    "plt.xticks([i + bar_width*1.5 for i in x_normal], categories)\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Occurrences Count')\n",
    "plt.title('Occurrences of Each Key in Different Categories')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparitive PWM/Motif Enrichment Across Enhancer Categories\n",
    "\n",
    "This analysis uses motifs and PWMs derived from the ENCODE database that were used in a FIMO run alongside FA files generated from high attention regions in `attent_explore.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the TSVs from FIMO that contain all of the PWM enrichment data.\n",
    "ae_motifs = pd.read_csv(\"./results/fimo/active/fimo.tsv\", sep=\"\\t\", header=0, skipfooter=3, skip_blank_lines=True, engine='python')\n",
    "peads_motifs = pd.read_csv(\"./results/fimo/peads/fimo.tsv\", sep=\"\\t\", header=0, skipfooter=3, skip_blank_lines=True, engine='python')\n",
    "peas_motifs = pd.read_csv(\"./results/fimo/peas/fimo.tsv\", sep=\"\\t\", header=0, skipfooter=3, skip_blank_lines=True, engine='python')\n",
    "\n",
    "negative_motifs = pd.read_csv(\"./results/fimo/neg/fimo.tsv\", sep=\"\\t\", header=0, skipfooter=3, skip_blank_lines=True, engine='python') # need to acquire negative set from biowulf to run a FIMO run \n",
    "neg_total_seq_len = (17 + 19 + 21 + 25 + 35) * 9000\n",
    "print(\"Total Sequence Length for Negative Hits: {}\".format(neg_total_seq_len))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantify enrichment we will divide the number of hits for each motif by the average sequence length of the high attention regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_grouped = ae_motifs.groupby('motif_id')\n",
    "peads_grouped = peads_motifs.groupby('motif_id')\n",
    "peas_grouped = peas_motifs.groupby('motif_id')\n",
    "\n",
    "negative_grouped = negative_motifs.groupby('motif_id')\n",
    "\n",
    "ae_pwm_map = {}\n",
    "\n",
    "def determine_length(df, custom_format = True):\n",
    "    all_seqs = (df['sequence_name']).unique()\n",
    "\n",
    "    total_length = 0\n",
    "    for seq in all_seqs:\n",
    "        if custom_format:\n",
    "            # sequence_start_end format\n",
    "            vals = seq.split('_')\n",
    "            seq_len = int(vals[2]) - int(vals[1])\n",
    "        else:\n",
    "            # chr:start-end format\n",
    "            coords = ((seq.split(\":\"))[1]).split('-')\n",
    "            seq_len = int(coords[1]) - int(coords[0])\n",
    "        total_length += seq_len\n",
    "    return total_length\n",
    "\n",
    "def create_pwm(grouped_df, length):\n",
    "    this_map = {}\n",
    "    for tf, group in grouped_df:\n",
    "        count = group.shape[0]\n",
    "        if pd.isnull(group.iloc[0][\"motif_alt_id\"]):\n",
    "            this_map[tf] = count / length\n",
    "        else:\n",
    "            this_map[group.iloc[0][\"motif_alt_id\"]] = count / length\n",
    "        # print(\"Unique Value {}, Count {}\".format(tf, count))\n",
    "    return this_map\n",
    "\n",
    "\n",
    "print([determine_length(i) for i in [ae_motifs, peads_motifs, peas_motifs]])\n",
    "print(determine_length(negative_motifs, False))\n",
    "\n",
    "ae_length = determine_length(ae_motifs)\n",
    "ae_map = create_pwm(ae_grouped, ae_length)\n",
    "\n",
    "peads_length = determine_length(peads_motifs)\n",
    "peads_map = create_pwm(peads_grouped, peads_length)\n",
    "\n",
    "peas_length = determine_length(peas_motifs)\n",
    "peas_map = create_pwm(peas_grouped, peas_length)\n",
    "\n",
    "negative_length = determine_length(negative_motifs, False)\n",
    "negative_map = create_pwm(negative_grouped, negative_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest = 'Egr1_primary'\n",
    "print(\"{} - Active: {}\\t PEADS: {}\\t PEAS: {}\\t Negative: {}\".format(\n",
    "    interest, ae_map[interest], peads_map[interest], peas_map[interest], negative_map[interest]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_interest = ['CTCF_full', 'KDM2B', 'Egr1_primary', 'NRF1_full', 'SP1', 'POU5F1', 'EP300.p2', 'BACH1', 'BACH2', 'MAX', 'REST.p3', 'MXI1', 'TEAD4', 'BRCA1', 'SRF', 'GABPA', 'CEBPB', 'SP4', 'MYC', 'USF2', 'SIX5', 'ZNF143.p2']\n",
    "\n",
    "def safe_find(mp, key):\n",
    "    if key not in mp:\n",
    "        return 0\n",
    "    else:\n",
    "        return mp[key]\n",
    "\n",
    "ae_motifscores = [safe_find(ae_map, motif) for motif in motifs_interest]\n",
    "peads_motifscores = [safe_find(peads_map, motif) for motif in motifs_interest]\n",
    "peas_motifscores = [safe_find(peas_map, motif) for motif in motifs_interest]\n",
    "negative_motifscores = [safe_find(negative_map, motif) for motif in motifs_interest]\n",
    "\n",
    "ae_motiffold = [(ae / neg) for ae, neg in zip(ae_motifscores, negative_motifscores)]\n",
    "peads_motiffold = [(ae / neg) for ae, neg in zip(peads_motifscores, negative_motifscores)]\n",
    "peas_motiffold = [(ae / neg) for ae, neg in zip(peas_motifscores, negative_motifscores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "pwms = pd.DataFrame({\n",
    "    \"Motifs\": motifs_interest,\n",
    "    # 'Negative': negative_motifscores,\n",
    "    'Active': ae_motiffold,\n",
    "    'PEADS': peads_motiffold,\n",
    "    'PEAS': peas_motiffold\n",
    "})\n",
    "\n",
    "pwms.set_index('Motifs', inplace=True)\n",
    "\n",
    "pwms_log = pwms.applymap(lambda x: -5 if x == 0 else -5 if x < 0 else round(math.log(x), 2))\n",
    "\n",
    "plt.figure(figsize=(6, 12))\n",
    "sns.heatmap(pwms, cmap='viridis', annot=True, fmt=\".2f\", vmax=8)\n",
    "# sns.heatmap(pwms, cmap='coolwarm', annot=True, fmt=\".2f\", center=1)\n",
    "plt.title('PWM Enrichment Heatmap Shown Through Fold Change')\n",
    "plt.xlabel('Enhancer Category')\n",
    "plt.ylabel('Motif')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metalearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
