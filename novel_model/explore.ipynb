{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0f5aa0-f1e4-4def-8632-381912da9cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from data_utils import tokenizer, SequenceDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc66c3e0-f52f-41eb-80bf-fe29d2ba0988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dir = \"../data/\"\n",
    "with open(results_dir + 'data.p', 'rb') as handle:\n",
    "    dataset = pickle.load(handle)\n",
    "\n",
    "## TODO:Load in the Test Set\n",
    "test_seq, test_lab = dataset['test_seq'], dataset['test_lab']\n",
    "test_mask = np.ones(test_seq.shape)\n",
    "test_dataset = SequenceDataset(test_seq, test_mask, test_lab, device=\"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f32eb79-d4d3-4f0d-8aa1-322d1c98a5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=4)\n",
    "# random_indices = random.sample(range(len(original_list)), 10)\n",
    "\n",
    "for i, data in list(enumerate(test_loader))[-5:]:\n",
    "    print(data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd01640-8629-4660-a743-4a35345e06b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10400.0\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset) / 4)\n",
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1470aca9-4354-4fad-8c78-8d7cc1389211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_weights = torch.FloatTensor([5.46, 1.0, 8.35])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def predict(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader):\n",
    "            vseqs, vlabels = data[\"Sequence\"], data[\"Class\"]\n",
    "            voutputs, vattn = model(vseqs, device)  # removed masks\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            total_loss += vloss\n",
    "            \n",
    "            predicted_labels = torch.argmax(voutputs, dim=1)\n",
    "            true_labels = torch.argmax(vlabels, dim=1)\n",
    "            total_correct += (predicted_labels == true_labels).sum().item()\n",
    "            total_samples += vlabels.size(dim=0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    average_loss = total_loss / len(loader)\n",
    "    \n",
    "#     n_classes = 3  # Number of classes\n",
    "#     true_labels_bin = label_binarize(true_labels, classes=np.arange(n_classes))\n",
    "\n",
    "#     # Calculate AUC for each class\n",
    "#     auc_scores = []\n",
    "#     for class_index in range(n_classes):\n",
    "#         # Treat current class as the positive class and the rest as negative\n",
    "#         class_probs = predicted_probs[:, class_index]\n",
    "#         class_labels = true_labels_bin[:, class_index]\n",
    "#         auc = roc_auc_score(class_labels, class_probs)\n",
    "#         auc_scores.append(auc)\n",
    "\n",
    "#     # Calculate average AUC across all classes\n",
    "#     average_?uc = np.mean(auc_scores)\n",
    "    print(accuracy, average_loss)\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d985e5da-3b35-4676-9439-6923cea74d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_one(sample, model):\n",
    "    sample = sample[1]\n",
    "    seq, label = sample['Sequence'], sample['Class']\n",
    "    logits, attention_scores = model(seq, torch.device('cpu'), predict=True)\n",
    "    print(logits, label)\n",
    "    print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cec8cc1-ca68-47e2-b3f9-c57be16f69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenoClassifier(\n",
       "  (transformer): GeneTransformer(\n",
       "    (embedding): Embedding(261, 144)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=144, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=144, bias=True)\n",
       "          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(144, 256, num_layers=4, batch_first=True)\n",
       "  (ff): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GenoClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "tkr = dataset['tokenizer']\n",
    "PATH = \"/data/Dcode/pranav/genoscanner/data/best_model_params128.pt\"\n",
    "ntokens = len(tkr.kmer2idx.keys())\n",
    "d_model = 144\n",
    "d_hidden = 256\n",
    "nlayers = 6\n",
    "nhead = 12\n",
    "dropout = 0.2\n",
    "num_classes = 3\n",
    "\n",
    "# Load in the Model and Load the Parameters from Training\n",
    "loaded_state_dict = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "model = GenoClassifier(ntokens, d_model, d_hidden, nlayers, nhead, dropout, num_classes) # Create an instance of your model\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Remove the \"module.\" prefix from the keys in the loaded state dictionary\n",
    "loaded_state_dict = {key.replace(\"module.\", \"\"): value for key, value in loaded_state_dict.items()}\n",
    "\n",
    "# Update the model's state dictionary with the loaded state dictionary\n",
    "model.load_state_dict(loaded_state_dict)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b5825e5-2470-40eb-acb8-5352211abc6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_loader = list(enumerate(test_loader))\n",
    "# predict_one(list_loader[10000], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e8a4589-abe6-4209-af42-045435df4305",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value -1: 264400 occurrences\n",
      "Value 0: 284386 occurrences\n",
      "Value 1: 173080 occurrences\n"
     ]
    }
   ],
   "source": [
    "train_seq, train_lab = dataset[\"train_seq\"], dataset[\"train_lab\"]\n",
    "train_mask = np.ones(train_seq.shape)\n",
    "# train_dataset = SequenceDataset(train_seq, train_mask, train_lab, device)\n",
    "\n",
    "counts = {\n",
    "    -1: np.count_nonzero(train_lab == -1),\n",
    "    0: np.count_nonzero(train_lab == 0),\n",
    "    1: np.count_nonzero(train_lab == 1)\n",
    "}\n",
    "\n",
    "# Print the counts\n",
    "for value, count in counts.items():\n",
    "    print(f\"Value {value}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eca8c8-c030-4a55-ac9d-89913afb5ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e6ffd-3b02-46ae-babb-d66748b82f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
